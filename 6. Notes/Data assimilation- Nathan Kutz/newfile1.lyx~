#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style british
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Data Assimilation
\end_layout

\begin_layout Standard
The integration of a model and measurements of a system to predict the future
 state of the system.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted1.png

\end_inset


\end_layout

\begin_layout Standard
Here the g(t,y) are M data measurements for each time point.
 This is an over determined system because there are M more constraints.
 In general, we use regression on over-determined problems.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted3.png

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $q_{1}$
\end_inset

 is an unknown dynamics (physics) that we don't know.
 Also, there is some error in the measurement of the initial data accounted
 by 
\begin_inset Formula $q_{2}$
\end_inset

.
 Also, the sensors have some noise depicted by 
\begin_inset Formula $q_{3}$
\end_inset

.
 We have to think about an optimal trajectory that minimises the model error
 
\begin_inset Formula $q_{1}$
\end_inset

, initial condition errors 
\begin_inset Formula $q_{2}$
\end_inset

 and in some sense the measurement errors 
\begin_inset Formula $q_{3}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted4.png

\end_inset


\end_layout

\begin_layout Standard

\series bold
We would have to define a optimisation function.
 Typically, a quadratic optimisation function is defined, so that we can
 apply convex optimisation techniques.
 Remember, that in optimisation module, we learnt, it is pretty easy to
 deal with quadratic functions.
 Also, in most of the cases, if the optimisation function is defined smarty
 (like 
\begin_inset Formula $x_{1}^{2}+x_{2}^{2}$
\end_inset

 not like 
\begin_inset Formula $x_{1}^{2}x_{2}^{2}$
\end_inset

), the Hessian matrix would be a constant and it would be either positive
 definite or non-positive definite at all the points.
 So, we don't have to bother about our initial guesses.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted5.png

\end_inset


\end_layout

\begin_layout Standard
J(y) is a quadratic loss function and 
\begin_inset Formula $W_{i}$
\end_inset

 are inverse of the error covariance.
 Individual terms would dropout if the model, initial condition or measurement
 data is perfect.
\end_layout

\begin_layout Section
Model prediction- an example
\end_layout

\begin_layout Standard
Let us assume that the model and the measurements are probability distributions
 as follows:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted6.png

\end_inset


\end_layout

\begin_layout Subsection
Model prediction with a Gaussian statistics
\end_layout

\begin_layout Subsubsection
Bayes theorem assuming Gaussian statistics
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted7.png

\end_inset


\end_layout

\begin_layout Standard
LHS is probablity of x given y, also known as the conditional probability.
 In the RHS, the p(y|x) is fairly easy to compute and p(x) and p(y) is already
 given.
\end_layout

\begin_layout Standard
Assuming the Gaussian statistics i.e.
 the area under the distribution would be 1.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted8.png

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $\sigma_{y}$
\end_inset

 is the variance in the x and y.
 Also, 
\begin_inset Formula $x_{0}$
\end_inset

 is the mean of x whereas, 
\begin_inset Formula $\sigma_{0}$
\end_inset

 is the variance of x and 
\begin_inset Formula $x_{0}$
\end_inset

.
 The resulting conditional probability would be as follows
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted9.png

\end_inset


\end_layout

\begin_layout Subsubsection
Computing the maximum likelihood error (MLE)
\end_layout

\begin_layout Standard

\series bold
Gaussian distribution are in exponentials and a log(maximum likelihood)
 cancels the exponentials and we are left with the quadratic function only.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted10.png

\end_inset


\end_layout

\begin_layout Standard
Here, J(x) is the maximum likelihood and log(
\begin_inset Formula $c_{3}$
\end_inset

) would cancel the 
\begin_inset Formula $c_{3}$
\end_inset

 from p(x|y) and we are left with just the quadratic functions.
\end_layout

\begin_layout Standard
So people always use Gaussian statistics for noise and the a log of the
 maximum likelihood gives the simple quadratic functions.
\end_layout

\begin_layout Standard

\series bold
Our objective is to find the x which minimises the J(x).
 
\series default
The most obvious thing that one can do is taking a derivative and find the
 stationary point.
 This is where we can leverage the quadratic form as the Hessian would be
 constant.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted11.png

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
This is the prediction of 
\begin_inset Formula $X$
\end_inset

 with an assimilation of the measurement data y defined by a probability
 distribution p(y).
 
\end_layout

\begin_layout Subsubsection*
Case 1: Measurements are perfect- sensor is perfect
\end_layout

\begin_layout Standard
The variance in p(y) would be zero i.e.
 
\begin_inset Formula $\sigma_{Y}=0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted12.png

\end_inset


\end_layout

\begin_layout Standard
It means if the measurements are perfect then the measurements are exactly
 the same as the state of the system.
 It is a consistency check.
\end_layout

\begin_layout Subsubsection*
Case 2: The Variance of the simulated system/error
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted13.png

\end_inset


\end_layout

\begin_layout Standard
So the variance of the data assimilated simulated system is less than the
 variance of the individual measurements i.e.
 variance of x and y.
 So the assimilated system guarantees that the error is less than the individual
 error in the measurement y and the state of the system x.
\end_layout

\end_body
\end_document
