#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Neural network from scratch on MNIST dataset
\end_layout

\begin_layout Standard
Link- https://www.samsonzhang.com/2020/11/24/understanding-the-math-behind-neural-
networks-by-building-one-from-scratch-no-tf-keras-just-numpy.html
\end_layout

\begin_layout Section
Dataset
\end_layout

\begin_layout Standard
The MNIST dataset is an acronym that stands for the Modified National Institute
 of Standards and Technology dataset.
 It is a dataset of 60,000 small square 28×28 pixel grayscale images of
 handwritten single digits between 0 and 9.
 The dataset we’re working with is the famous MNIST handwritten digit dataset,
 commonly used for instructive ML and computer vision projects.
 It contains 28 x 28 grayscale images of handwritten digits that look like
 this.
 Each image is accompanied by a label of what digit it belongs to, from
 0 to 9.
 Our task is to build a network that takes in an image like this and predicts
 what digit is written in it.
\end_layout

\begin_layout Section
Neural network overview
\end_layout

\begin_layout Standard
Our network will have three layers total: an input layer and two layers
 with parameters.
 Because the input layer has no parameters, this network would be referred
 to as a two-layer neural network.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted1.png

\end_inset


\end_layout

\begin_layout Standard
The input layer has 784 nodes, corresponding to each of the 784 pixels in
 the 28x28 input image.
 Each pixel has a value between 0 and 255, with 0 being black and 255 being
 white.
 It’s common to normalize these values — getting all values between 0 and
 1, here by simply dividing by 255 — before feeding them in the network.
\end_layout

\begin_layout Standard
The second layer, or hidden layer, could have any amount of nodes, but we’ve
 made it really simple here with just 10 nodes.
 The value of each of these nodes is calculated based on weights and biases
 applied to the value of the 784 nodes in the input layer.
 After this calculation, a ReLU activation is applied to all nodes in the
 layer (more on this later).
\end_layout

\begin_layout Standard
In a deeper network, there may be multiple hidden layers back to back before
 the output layer.
 In this network, we’ll only have one hidden layer before going to the output.
\end_layout

\begin_layout Standard
The output layer also has 10 nodes, corresponding to each of the output
 classes (digits 0 to 9).
 The value of each of these nodes will again be calculated from weights
 and biases applied to the value of the 10 nodes in the hidden layer, with
 a softmax activation applied to them to get the final output.
\end_layout

\begin_layout Subsection
The gradient descent in backprop
\end_layout

\begin_layout Standard
The process of taking an image input and running through the neural network
 to get a prediction is called forward propagation.
 The prediction that is made from a given image depends on the weights and
 biases, or parameters, of the network.
\end_layout

\begin_layout Standard
To train a neural network, then, we need to update these weights and biases
 to produce accurate predictions.
 We do this through a process called gradient descent.
 The basic idea of gradient descent is to figure out what direction each
 parameter can go in to decrease error by the greatest amount, then nudge
 each parameter in its corresponding direction over and over again until
 the parameters for minimum error and highest accuracy are found.
 Check out my visualization of gradient descent here.
\end_layout

\begin_layout Standard
In a neural network, gradient descent is carried out via a process called
 backward propagation, or backprop.
 In backprop, instead of taking an input image and running it forwards through
 the network to get a prediction, we take the previously made prediction,
 calculate an error of how off it was from the actual value, then run this
 error backwards through the network to find out how much each weight and
 bias parameter contributed to this error.
 Once we have these error derivative terms, we can nudge our weights and
 biases accordingly to improve our model.
 Do it enough times, and we’ll have a neural network that can recognize
 handwritten digits accurately.
\end_layout

\begin_layout Section
The mathematics
\end_layout

\begin_layout Subsection
representing the data
\end_layout

\begin_layout Standard
These vectors can be stacked together in a matrix to carry out vectorized
 calculations.
 That is, instead of using a for loop to go over all training examples,
 we can calculate error from all examples at once with matrix operations.
\end_layout

\begin_layout Standard
In most contexts, including for machine learning, the convention is to stack
 these vectors as rows of the matrix, giving the matrix dimensions of m
 rows × n columns , where m is the number of training examples and n is
 the number of features, in this case 784.
 To make our math easier, we’re going to transpose this matrix, giving it
 dimensions n × m instead, with each column corresponding to a training
 example and each row a training feature.
\end_layout

\begin_layout Subsection
Representing weights and biases
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted2.png

\end_inset


\end_layout

\begin_layout Section
Forward propagation
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted3.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted4.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted5.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted6.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted7.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted8.png

\end_inset


\end_layout

\begin_layout Section
Backward propagation (gradient descent)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted9.png

\end_inset


\end_layout

\begin_layout Subsection
One-hot encoding
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted10.png

\end_inset


\end_layout

\begin_layout Subsection
Gradient descent
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted11.png

\end_inset


\end_layout

\begin_layout Subsubsection
Backprop (computing the partial derivatives of the cost fucntion wrt to
 weights and bias in the network)
\end_layout

\begin_layout Subsubsection*
Assumption 1: The first assumption we need is that the cost function can
 be written as an average over cost functions for individual training examples,
 x.
 This is the case for the quadratic cost function.
 This assumption will also hold true for all the other cost functions
\end_layout

\begin_layout Subsubsection*
Assumtion 2: The second assumption we make about the cost is that it can
 be written as a function of the outputs from the neural network.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted12.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted13.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted14.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted15.png

\end_inset


\end_layout

\begin_layout Section
Recap to neural network
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted16.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted18.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted19.png

\end_inset


\end_layout

\begin_layout Section
Full algorithm
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted17.png

\end_inset


\end_layout

\end_body
\end_document
